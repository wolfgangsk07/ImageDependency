import osimport argparseimport loggingfrom datetime import datetime, timedeltafrom tqdm import tqdmimport warningswarnings.filterwarnings("ignore", module="timm")import pickleimport h5pyimport wandbimport randomimport torchimport sysimport numpy as npimport pandas as pdimport torch.nn as nnfrom timm.layers import DropPath, trunc_normal_from torch.utils.data import DataLoaderfrom sklearn.metrics import mean_absolute_errorfrom scipy.stats import pearsonr# 项目路径配置module_path = "/yourworkspace/"sys.path.insert(0, module_path)from script.src.agentAttention import AgentAttentionfrom script.src.Calculation import compute_correlations,smape,evaluatefrom script.src.GeneExpressionMLP import GeneExpressionMLPfrom script.src.read_data import SuperTileRNADatasetfrom script.src.tformer_lin import Tformer_linfrom script.src.utils import setup_logger,patient_kfold, custom_collate_fn, seed_workerfrom script.src.tformer import Tformerdef train_model(model, dataloaders, optimizer, save_dir, accelerator=None,                num_epochs=200, patience=20, verbose=True, phases=['train', 'val'],                 split=None, save_on='loss', stop_on='loss', delta=0.5, logger=None):    """模型训练函数"""    # 创建保存目录    os.makedirs(save_dir, exist_ok=True)    save_path = os.path.join(save_dir, f'model_best_{split}.pt') if split else os.path.join(save_dir, 'model_best.pt')        loss_fn = nn.MSELoss()    epoch_since_best = 0    best_loss = float('inf')    best_score = 0        # 训练历史记录    history = {        'train_loss': [], 'val_loss': [],        'train_mae': [], 'val_mae': [],        'train_score': [], 'val_score': []    }        if logger:        logger.info(f"Starting training for {num_epochs} epochs with patience {patience}")        logger.info(f"Model will be saved to: {save_path}")    for epoch in range(num_epochs):        epoch_metrics = {phase: {'loss': [], 'mae': [], 'score': []} for phase in phases}                for phase in phases:            model.train() if phase == 'train' else model.eval()            # 获取模型设备 - 使用标准PyTorch方式            device = next(model.parameters()).device            for images, omics, _, _ in tqdm(dataloaders[phase], desc=f"Epoch {epoch} {phase}"):                # 跳过空批次                if images.nelement() == 0:                    continue                                images = images.to(device)                omics = omics.to(device)                                with torch.set_grad_enabled(phase == 'train'):                    preds = model(images)                    loss = loss_fn(preds, omics)                                # 计算指标                mae = mean_absolute_error(omics.cpu().numpy(), preds.detach().cpu().numpy())                score = compute_correlations(omics.cpu().numpy(), preds.detach().cpu().numpy())                                # 记录指标                epoch_metrics[phase]['loss'].append(loss.item())                epoch_metrics[phase]['mae'].append(mae)                epoch_metrics[phase]['score'].append(score)                                # 训练步骤                if phase == 'train':                    optimizer.zero_grad()                    loss.backward()                    optimizer.step()                        # 计算平均指标            for metric in ['loss', 'mae', 'score']:                values = epoch_metrics[phase][metric]                avg_value = np.mean(values) if values else float('nan')                history[f'{phase}_{metric}'].append(avg_value)                # 打印指标        if verbose and logger:            logger.info(f"\nEpoch {epoch + 1}/{num_epochs}")            for phase in phases:                logger.info(f"{phase.upper()}: Loss: {history[f'{phase}_loss'][-1]:.4f}, "                            f"MAE: {history[f'{phase}_mae'][-1]:.4f}")                # 验证阶段模型保存        val_loss = history['val_loss'][-1]        val_score = history['val_score'][-1]                if not np.isnan(val_loss) and val_loss < best_loss:            best_loss = val_loss            epoch_since_best = 0            torch.save(model.state_dict(), save_path)            if logger:                logger.info(f"Saved best model (loss={val_loss:.4f}) at epoch {epoch+1}")        else:            epoch_since_best += 1                # 早停检查        if epoch_since_best >= patience:            if logger:                logger.info(f"Early stopping triggered at epoch {epoch+1}!")            break        return model, historydef get_model(model_type, feature_dim, num_clusters, num_outputs, num_heads, depth, device, logger=None):    """根据类型创建模型"""    if model_type == 'tformer':        model = Tformer(num_outputs=num_outputs, dim=feature_dim, depth=depth,                   heads=num_heads, num_clusters=num_clusters,                   mlp_dim=2048, dim_head=64)    elif model_type == 'tformer_lin':        model = Tformer_lin(num_outputs=num_outputs, input_dim=feature_dim, depth=depth,                   nheads=num_heads, num_clusters=num_clusters,                   dimensions_f=64, dimensions_c=64, dimensions_s=64)    elif model_type == 'Agent':        model = AgentAttention(num_outputs=num_outputs, num_patches=num_clusters,                              dim=feature_dim, num_heads=num_heads)    elif model_type == 'MLP':        model = GeneExpressionMLP(dim=feature_dim, num_patches=num_clusters,                                 num_outputs=num_outputs)    else:        raise ValueError(f"Unknown model type: {model_type}")        # 确保模型移动到指定设备    model = model.to(device)        if logger:        param_count = sum(p.numel() for p in model.parameters())        logger.info(f"Initialized {model_type} model with {param_count/1e6:.2f}M parameters")        return modeldef format_timedelta(delta):    """将时间差格式化为可读的字符串形式"""    total_seconds = int(delta.total_seconds())    hours, remainder = divmod(total_seconds, 3600)    minutes, seconds = divmod(remainder, 60)    return f"{hours}小时 {minutes}分钟 {seconds}秒"def main(args):    """主函数"""    # 记录整体开始时间    total_start_time = datetime.now()        # 设置日志记录器    log_dir = os.path.join(module_path, "logs", args.cancer_type)    logger, log_path = setup_logger(log_dir, args.cancer_type, args.prediction_type)    logger.info(f"Logging started at {total_start_time.strftime('%Y-%m-%d %H:%M:%S')}")    logger.info(f"Log file: {log_path}")        # 记录配置参数    logger.info("\n" + "=" * 50)    logger.info("实验配置:")    for arg in vars(args):        logger.info(f"{arg:>15}: {getattr(args, arg)}")    logger.info("=" * 50 + "\n")        # 设置随机种子    random.seed(args.seed)    np.random.seed(args.seed)    torch.manual_seed(args.seed)    torch.backends.cudnn.deterministic = True    logger.info(f"Set random seed to {args.seed}")        # 设备配置    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    logger.info(f"Using device: {device}")        # 文件路径配置    base_dir = "/backup/lgx/path_omics_t"    ref_csv = os.path.join(base_dir, "data", "reference", "dependency", f"{args.cancer_type}_ref.csv")    feature_path = os.path.join(base_dir, "data", "result", "cluster", args.extraction_model, args.cancer_type)    save_dir = os.path.join(base_dir, "data", "result", "model", args.extraction_model, args.prediction_type, args.cancer_type)        os.makedirs(save_dir, exist_ok=True)    logger.info(f"Reference CSV: {ref_csv}")    logger.info(f"Feature path: {feature_path}")    logger.info(f"Save directory: {save_dir}")        # 加载数据    try:        df = pd.read_csv(ref_csv)        logger.info(f"Loaded reference data with shape: {df.shape}")    except Exception as e:        logger.error(f"Failed to load reference data: {e}")        raise        train_idxs, val_idxs, test_idxs = patient_kfold(df, n_splits=5)    logger.info(f"Generated patient splits with {len(train_idxs)} folds")        # 结果存储    test_results_splits = {}        # 数据生成器    g = torch.Generator()    g.manual_seed(args.seed)        # 记录每个fold的开始时间    fold_times = []        # 交叉验证循环    for fold_idx, (train_idx, val_idx, test_idx) in enumerate(zip(train_idxs, val_idxs, test_idxs)):        fold_start_time = datetime.now()        logger.info(f"\n{'='*40}")        logger.info(f"Processing Fold {fold_idx + 1}/5")        logger.info(f"开始时间: {fold_start_time.strftime('%Y-%m-%d %H:%M:%S')}")        logger.info(f"{'='*40}")        logger.info(f"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}, Test samples: {len(test_idx)}")                # 创建数据集        train_df = df.iloc[train_idx]        val_df = df.iloc[val_idx]        test_df = df.iloc[test_idx]                # 保存患者ID        for name, data in zip(["train", "val", "test"], [train_df, val_df, test_df]):            np.save(os.path.join(save_dir, f"{name}_{fold_idx}.npy"), np.unique(data.patient_id))                # 创建数据集        datasets = {}        for name, data_df in zip(["train", "val", "test"], [train_df, val_df, test_df]):            datasets[name] = SuperTileRNADataset(                data_df, feature_path, args.extraction_model, args.cluster_type, args.cancer_project            )            logger.info(f"{name} dataset size: {len(datasets[name])}")                # 获取输入输出维度        num_outputs = datasets["train"].num_genes        feature_dim = datasets["train"].feature_dim        logger.info(f"Feature dimension: {feature_dim}, Number of genes: {num_outputs}")                # 创建数据加载器        dataloaders = {}        for name in ["train", "val", "test"]:            dataloaders[name] = DataLoader(                datasets[name],                batch_size=args.batch_size,                shuffle=(name == "train"),                num_workers=4,                pin_memory=True,                collate_fn=custom_collate_fn,                worker_init_fn=seed_worker,                generator=g            )            logger.info(f"{name} dataloader batches: {len(dataloaders[name])}")                # 初始化模型        model = get_model(            model_type=args.prediction_type,            feature_dim=feature_dim,            num_clusters=args.num_clusters,            num_outputs=num_outputs,            num_heads=args.num_heads,            depth=args.depth,            device=device,            logger=logger        )                # 优化器        optimizer = torch.optim.AdamW(            model.parameters(),            lr=args.lr,            weight_decay=1e-4        )        logger.info(f"Optimizer: AdamW with lr={args.lr}, weight_decay=1e-4")                # 训练模型        model, history = train_model(            model=model,            dataloaders={"train": dataloaders["train"], "val": dataloaders["val"]},            optimizer=optimizer,            save_dir=save_dir,            num_epochs=args.num_epochs,            patience=args.patience,            split=fold_idx,            save_on=args.save_on,            stop_on=args.stop_on,            logger=logger        )                # 评估模型        logger.info(f"Evaluating fold {fold_idx} on test set")        preds, real, wsis, projs = evaluate(model, dataloaders["test"], suff=f'_{fold_idx}', logger=logger)                # 基线模型评估        logger.info("Evaluating baseline/random model on test set")        random_model = get_model(            model_type=args.prediction_type,            feature_dim=feature_dim,            num_clusters=args.num_clusters,            num_outputs=num_outputs,            num_heads=args.num_heads,            depth=args.depth,            device=device        )        random_model.to(device)        random_preds, _, _, _ = evaluate(random_model, dataloaders["test"], suff=f'_{fold_idx}_rand', logger=logger)                # 存储结果        test_results_splits[f'split_{fold_idx}'] = {            'real': real,            'preds': preds,            'random': random_preds,            'wsi_file_name': wsis,            'project': projs        }                # 记录fold结束时间        fold_end_time = datetime.now()        fold_duration = fold_end_time - fold_start_time        fold_times.append(fold_duration)                logger.info(f"Fold {fold_idx + 1} 完成, 耗时: {format_timedelta(fold_duration)}")        logger.info(f"Fold {fold_idx + 1} 结束时间: {fold_end_time.strftime('%Y-%m-%d %H:%M:%S')}")        logger.info(f"{'='*40}\n")        # 记录整体结束时间    total_end_time = datetime.now()    total_duration = total_end_time - total_start_time        # 保存最终结果    test_results_splits['genes'] = list(df.columns[1:])    results_path = os.path.join(save_dir, 'test_results.pkl')        try:        with open(results_path, 'wb') as f:            pickle.dump(test_results_splits, f, protocol=pickle.HIGHEST_PROTOCOL)        logger.info(f"Saved test results to {results_path}")    except Exception as e:        logger.error(f"Failed to save test results: {e}")        # 记录总运行时间    logger.info("\n" + "=" * 50)    logger.info("整体运行时间统计")    logger.info(f"开始时间: {total_start_time.strftime('%Y-%m-%d %H:%M:%S')}")    logger.info(f"结束时间: {total_end_time.strftime('%Y-%m-%d %H:%M:%S')}")    logger.info(f"总耗时: {format_timedelta(total_duration)}")        # 记录每个fold的耗时    logger.info("\n各个Fold耗时统计:")    for i, duration in enumerate(fold_times):        logger.info(f"Fold {i+1}: {format_timedelta(duration)}")        avg_fold_time = sum(fold_times, timedelta(0)) / len(fold_times)    logger.info(f"平均每个Fold耗时: {format_timedelta(avg_fold_time)}")    logger.info("=" * 50 + "\n")        logger.info("训练和评估完成!")    # 清理日志处理器    handlers = logger.handlers[:]    for handler in handlers:        handler.close()        logger.removeHandler(handler)#tformer_lin = vis #tformer = vitif __name__ == "__main__":    parser = argparse.ArgumentParser(description="基因表达预测模型训练")        # 实验参数    parser.add_argument("--cancer_type", type=str, default="BLCA", help="癌症类型，如BLCA")    parser.add_argument("--cancer_project", type=str, default="TCGA", help="癌症项目，如TCGA")    parser.add_argument("--prediction_type", type=str, default="tformer_lin",                         choices=["tformer", "tformer_lin", "Agent", "MLP"], help="预测模型类型")    parser.add_argument("--extraction_model", type=str, default="vit", choices=["vit", "resnet"],help="特征提取模型")    parser.add_argument("--cluster_type", type=str, default="KMeans", help="聚类类型")        # 模型参数    parser.add_argument("--batch_size", type=int, default=4, help="批量大小")    parser.add_argument("--num_clusters", type=int, default=900, help="聚类数量")    parser.add_argument("--num_heads", type=int, default=16, help="多头注意力头数")    parser.add_argument("--depth", type=int, default=8, help="模型深度/层数")        # 训练参数    parser.add_argument("--lr", type=float, default=1e-3, help="学习率")    parser.add_argument("--num_epochs", type=int, default=200, help="训练轮数")    parser.add_argument("--patience", type=int, default=50, help="早停耐心值")    parser.add_argument("--save_on", type=str, default="loss",                         choices=["loss", "loss+corr"], help="模型保存条件")    parser.add_argument("--stop_on", type=str, default="loss",                         choices=["loss", "loss+corr"], help="早停条件")        # 其他参数    parser.add_argument("--seed", type=int, default=999, help="随机种子")    args = parser.parse_args()    main(args)